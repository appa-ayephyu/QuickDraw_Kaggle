{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Black strokes CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7f2a9516140a84124bf7bbf538ee4c30860b778"
   },
   "source": [
    "## packages\n",
    "Import the necessary libraries and a few helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DP_DIR = '../input/shuffle-csv2s/'\n",
    "INPUT_DIR = '../input/quickdraw-doodle-recognition/'\n",
    "\n",
    "BASE_SIZE = 256\n",
    "NCSVS = 100\n",
    "NCATS = 340\n",
    "np.random.seed(seed=1957)\n",
    "tf.set_random_seed(seed=1787)\n",
    "\n",
    "def f2cat(filename: str) -> str:\n",
    "    return filename.split('.')[0]\n",
    "\n",
    "def list_all_categories():\n",
    "    files = os.listdir(os.path.join(INPUT_DIR, 'train_simplified'))\n",
    "    return sorted([f2cat(f) for f in files], key=str.lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ce6d2aa7de1fa341144def7d3a5b1ffdea26bc91"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import os\n",
    "import ast\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "plt.rcParams['font.size'] = 14\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "start = dt.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metric from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b2fcd1a08ae1ae0619be38a113a244eb6515b63b"
   },
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=3):\n",
    "    \"\"\"\n",
    "    Source: https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "    \"\"\"\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=3):\n",
    "    \"\"\"\n",
    "    Source: https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "def preds2catids(predictions):\n",
    "    return pd.DataFrame(np.argsort(-predictions, axis=1)[:, :3], columns=['a', 'b', 'c'])\n",
    "\n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "264156422a95e4b350886d558d516ae8bd2e25c0"
   },
   "source": [
    "## Test parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "54e5f0c637195b6624e2f3e6db5e7f8990e14eb7"
   },
   "outputs": [],
   "source": [
    "STEPS = 4800\n",
    "EPOCHS = 18\n",
    "size = 64\n",
    "batchsize = 340"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "0860ec35bee03f0c5cd21202dc7471c2d201cf5f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cnnlayer(size, conv_layers, dense_layers,\n",
    "                      dense_dropout):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Conv2D(conv_layers[0], kernel_size=(7, 7), padding='same', activation='relu', input_shape=(size, size, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(\n",
    "        Conv2D(conv_layers[0], kernel_size=(5, 5), padding='same', activation='relu', input_shape=(size, size, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(\n",
    "        Conv2D(conv_layers[0], kernel_size=(3, 3), padding='same', activation='relu', input_shape=(size, size, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "     \n",
    "    model.add(\n",
    "    Conv2D(conv_layers[0], kernel_size=(3, 3), padding='same', activation='relu', input_shape=(size, size, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for dense_layer_size in dense_layers:\n",
    "        model.add(Dense(dense_layer_size, activation='relu'))\n",
    "        model.add(Activation('relu'))\n",
    "        if dense_dropout:\n",
    "            model.add(Dropout(dense_dropout))\n",
    "\n",
    "    model.add(Dense(NCATS, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
    "\n",
    "## model parameters\n",
    "model = cnnlayer(size=size,\n",
    "                          conv_layers=[128,64],\n",
    "                          dense_layers=[1024],\n",
    "                          dense_dropout=0.20)\n",
    "model.compile(optimizer=Adam(lr=0.0004), loss='categorical_crossentropy',\n",
    "              metrics=[categorical_crossentropy, categorical_accuracy, top_3_accuracy])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab1834ea2757a53d602a3508efffcc34bc190dc7"
   },
   "source": [
    "## Image Generator and Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f6455bf9555b8381b6a4292098a64a0eb7ff54dc"
   },
   "outputs": [],
   "source": [
    "def draw_cv2(raw_strokes, size=256, lw=3, time_color=True):\n",
    "    img = np.zeros((BASE_SIZE, BASE_SIZE), np.uint8)\n",
    "    for t, stroke in enumerate(raw_strokes):\n",
    "        for i in range(len(stroke[0]) - 1):\n",
    "            color = 255 - min(t, 10) * 13 if time_color else 255\n",
    "            _ = cv2.line(img, (stroke[0][i], stroke[1][i]),\n",
    "                         (stroke[0][i + 1], stroke[1][i + 1]), color, lw)\n",
    "    if size != BASE_SIZE:\n",
    "        return cv2.resize(img, (size, size))\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "def image_generator_xd(size, batchsize, ks, lw=3, time_color=True):\n",
    "    while True:\n",
    "        for k in np.random.permutation(ks):\n",
    "            filename = os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(k))\n",
    "            for df in pd.read_csv(filename, chunksize=batchsize):\n",
    "                df['drawing'] = df['drawing'].apply(ast.literal_eval)\n",
    "                x = np.zeros((len(df), size, size, 1))\n",
    "                for i, raw_strokes in enumerate(df.drawing.values):\n",
    "                    x[i, :, :, 0] = draw_cv2(raw_strokes, size=size, lw=lw,\n",
    "                                             time_color=time_color)\n",
    "                    x[i, :, :, 0] = draw_cv2(raw_strokes, size=size, lw=lw,\n",
    "                                             time_color=time_color)\n",
    "                    (h,w) = x[i, :, :, 0].shape[:2]\n",
    "                    center = (w / 2, h / 2)\n",
    "                    M = cv2.getRotationMatrix2D(center, np.random.randint(180), 1.0)\n",
    "                    x[i, :, :, 0] = cv2.warpAffine(x[i, :, :, 0], M, (w, h))\n",
    "                x = preprocess_input(x).astype(np.float32)\n",
    "                y = keras.utils.to_categorical(df.y, num_classes=NCATS)\n",
    "                yield x, y\n",
    "\n",
    "def df_to_image_array_xd(df, size, lw=3, time_color=True):\n",
    "    df['drawing'] = df['drawing'].apply(ast.literal_eval)\n",
    "    x = np.zeros((len(df), size, size, 1))\n",
    "    for i, raw_strokes in enumerate(df.drawing.values):\n",
    "        x[i, :, :, 0] = draw_cv2(raw_strokes, size=size, lw=lw, time_color=time_color)\n",
    "    x = preprocess_input(x).astype(np.float32)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generating of validation array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98ff512e1a1b5e86e86d9eef4127525bedf3b9e1"
   },
   "outputs": [],
   "source": [
    "validation_df = pd.read_csv(os.path.join(DP_DIR, 'train_k{}.csv.gz'.format(NCSVS - 1)), nrows=100000)\n",
    "x_validation = df_to_image_array_xd(valid_df, size)\n",
    "y_validation = keras.utils.to_categorical(valid_df.y, num_classes=NCATS)\n",
    "print(x_validation.shape, y_validation.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d80ad7f4d378ea7f30479221d604eeeed559cae4"
   },
   "outputs": [],
   "source": [
    "train_datagen = image_generator_xd(size=size, batchsize=batchsize, ks=range(NCSVS - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot to display pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ce5fb89fbb77777316d6fca7689b6636c0e6021"
   },
   "outputs": [],
   "source": [
    "x, y = next(train_datagen)\n",
    "n = 8\n",
    "fig, axs = plt.subplots(nrows=n, ncols=n, sharex=True, sharey=True, figsize=(12, 12))\n",
    "for i in range(n**2):\n",
    "    ax = axs[i // n, i % n]\n",
    "    (-x[i]+1)/2\n",
    "    ax.imshow((-x[i, :, :, 0] + 1)/2, cmap=plt.cm.gray)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "fig.savefig('gs.png', dpi=600)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "_uuid": "da72d70fc1781e80427d45a80c07b3571dda0b36",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## tensorflow function to reduce Lr rate on plateau after 3 epoch\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=3,\n",
    "                      min_delta=0.0005, mode='max', cooldown=3, verbose=1)\n",
    "]\n",
    "hists = []\n",
    "hist = model.fit_generator(\n",
    "    train_datagen, steps_per_epoch=STEPS, epochs=EPOCHS, verbose=1,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks = callbacks\n",
    ")\n",
    "hists.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05767778d356bc63b7cded355159fd4082eee1a5"
   },
   "outputs": [],
   "source": [
    "hist_df = pd.concat([pd.DataFrame(hist.history) for hist in hists], sort=True)\n",
    "hist_df.index = np.arange(1, len(hist_df)+1)\n",
    "fig, axs = plt.subplots(nrows=2, sharex=True, figsize=(16, 10))\n",
    "axs[0].plot(hist_df.val_categorical_accuracy, lw=5, label='Validation Accuracy')\n",
    "axs[0].plot(hist_df.categorical_accuracy, lw=5, label='Training Accuracy')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].grid()\n",
    "axs[0].legend(loc=0)\n",
    "axs[1].plot(hist_df.val_categorical_crossentropy, lw=5, label='Validation MLogLoss')\n",
    "axs[1].plot(hist_df.categorical_crossentropy, lw=5, label='Training MLogLoss')\n",
    "axs[1].set_ylabel('MLogLoss')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].grid()\n",
    "axs[1].legend(loc=0)\n",
    "fig.savefig('hist.png', dpi=600)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictions = model.predict(x_valid, batch_size=128, verbose=1)\n",
    "map3 = mapk(valid_df[['y']].values, preds2catids(valid_predictions).values)\n",
    "print('Map3: {:.3f}'.format(map3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be4577a9ba00611697eea8f241a42c504981e86f"
   },
   "source": [
    "## Export test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7d14348150baf753e90cf2719b9f31dd564f6a2"
   },
   "outputs": [],
   "source": [
    "\n",
    "TEST_DIR='../input/testingfile2/'\n",
    "filename = os.path.join(os.path.join(TEST_DIR, 'train_k0.csv.gz'))\n",
    "for df in pd.read_csv(filename, chunksize=batchsize):\n",
    "        x_test = df_to_image_array_xd(df, size)\n",
    "        test_predictions = model.predict(x_test, batch_size=128, verbose=1)\n",
    "        prob_list = test_predictions.data.tolist();\n",
    "        top = np.argsort(-test_predictions,1)[:,:10]\n",
    "        top_list =  np.sort(-test_predictions,1)[:,:10] \n",
    "        top_list = np.multiply(top_list,-1)\n",
    "        dfinal = pd.DataFrame({ 'key_id' : df['key_id'] , 'scores' : top_list.data.tolist(), 'labels': top.data.tolist()}).astype(str)\n",
    "        dfinal.to_csv('bw_cnn_submission.csv', mode='a', header=False, index=False, columns=['key_id', 'scores','labels'], compression='')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b418f4c06c4e4453aa1b5ab16dde344eb8b735c5"
   },
   "outputs": [],
   "source": [
    "end = dt.datetime.now()\n",
    "print('Latest run {}.\\nTotal time {}s'.format(end, (end - start).seconds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
